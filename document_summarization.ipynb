{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506d5464",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Dataset: Synthetic data (sample news articles)\n",
    "documents = [\n",
    "    \"The stock market saw a significant drop today as several tech companies reported lower earnings than expected.\",\n",
    "    \"Researchers have discovered a new species of frog in the Amazon rainforest that is capable of changing its skin color.\",\n",
    "    \"The local government has announced new measures to tackle the rising pollution levels in the city.\",\n",
    "    \"A recent study shows that regular exercise can reduce the risk of heart disease and improve mental health.\",\n",
    "    \"Tech companies are facing increased scrutiny over privacy concerns as data breaches continue to occur.\",\n",
    "    \"The new policy aims to support small businesses by providing tax relief and access to low-interest loans.\"\n",
    "]\n",
    "\n",
    "# Preprocess text\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "def preprocess(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", '', text)  # Remove punctuation\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "processed_docs = [preprocess(doc) for doc in documents]\n",
    "processed_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28866e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_docs)\n",
    "tfidf_matrix.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffcfbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Compute cosine similarity\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "similarity_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06dd725",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Create a graph using similarity matrix\n",
    "graph = nx.from_numpy_array(similarity_matrix)\n",
    "\n",
    "# Step 4: Apply PageRank algorithm for scoring sentences\n",
    "scores = nx.pagerank(graph)\n",
    "scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e71c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: Rank sentences and create summary\n",
    "ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(documents)), reverse=True)\n",
    "summary = ' '.join([ranked_sentences[i][1] for i in range(2)])  # Top 2 sentences as summary\n",
    "\n",
    "# Output the summary\n",
    "print(\"Document Summary:\")\n",
    "print(summary)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
